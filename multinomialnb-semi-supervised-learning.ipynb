{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1360147,"sourceType":"datasetVersion","datasetId":792115}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n# Multinomial Naive Bayes | Semi supervised learning \n\n\n\n**Problem statement**: The problem requires us to predict the author, i.e. EAP, HPL and MWS given the text. In simpler words, text classification with 3 different classes.\n\n**Evaluation metric**:  Multi-class logarithmic loss\n\n**Objective**: Using semi-supervised learning as an approach to the given NLP problem and comparing it to the supervised learning approach.(based on the given evaluation metric)\n\n**Dataset**:We have train and test dataset.\n\n**Approach:**\nFor this problem we will use **Multinomial Naive Bayes** as our classification algorithm.We will use both **TF-IDF** and **CountVectorizer** to convert the terms into numeric values.\n\nSteps we will follow:\n1. Split the dataset into train,valid and unlabeled\n2. Fitting the TF-IDF to both train and valid datasets\n3. Using the labeled dataset to train our model\n4. Predictions made on the unlabeled dataset\n5. Use the predicted values as pseudo labels and combine with the train dataset\n6. Train the model on this dataset and evaluate on valid dataset\n7. Repeat steps 2-6 using CountVectorizer to convert the terms to numeric value/count \n\n\n**Note**: First we use a simple model for semi- supervised learning as here we don't have actual unlabeled dataset so we use self training where we divide the dataset intothree parts: train ,valid and unalabeled.In the unlabeled we drop the target column.\n\nWe will then use **self training classifier** from scikit learn's semi supervised module\n\n\nReference:\n\n* https://www.kaggle.com/code/abhishek/approaching-almost-any-nlp-problem-on-kaggle\n* https://www.kaggle.com/code/sasakitetsuya/semi-supervised-classification-on-a-text-dataset\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Simple semi-supervised learning model\n\nFirst we have used a simple model of semi supervised learning without using any specific semi supervised learning technique","metadata":{}},{"cell_type":"code","source":"# Importing libraries \n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\nfrom sklearn import model_selection, preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.semi_supervised import LabelSpreading, SelfTrainingClassifier\nfrom sklearn.preprocessing import FunctionTransformer\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:25:24.166284Z","iopub.execute_input":"2024-02-22T18:25:24.166816Z","iopub.status.idle":"2024-02-22T18:25:24.174605Z","shell.execute_reply.started":"2024-02-22T18:25:24.166759Z","shell.execute_reply":"2024-02-22T18:25:24.173534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loading dataset\n\ntrain=pd.read_csv('/kaggle/input/spooky/train.csv')\ntest=pd.read_csv('/kaggle/input/spooky/test.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:25:26.540776Z","iopub.execute_input":"2024-02-22T18:25:26.541192Z","iopub.status.idle":"2024-02-22T18:25:26.636077Z","shell.execute_reply.started":"2024-02-22T18:25:26.541162Z","shell.execute_reply":"2024-02-22T18:25:26.634689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#looking at the data\n\ntrain.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:25:28.879750Z","iopub.execute_input":"2024-02-22T18:25:28.880476Z","iopub.status.idle":"2024-02-22T18:25:28.892454Z","shell.execute_reply.started":"2024-02-22T18:25:28.880441Z","shell.execute_reply":"2024-02-22T18:25:28.891330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we use the LabelEncoder from scikit-learn to convert text labels to integers: 0,1,2\n\nlbl_enc= preprocessing.LabelEncoder()\ny=lbl_enc.fit_transform(train.author.values)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:25:31.682976Z","iopub.execute_input":"2024-02-22T18:25:31.683382Z","iopub.status.idle":"2024-02-22T18:25:31.696491Z","shell.execute_reply.started":"2024-02-22T18:25:31.683352Z","shell.execute_reply":"2024-02-22T18:25:31.695160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logloss function\n\ndef multiclass_logloss(actual,predicted,eps=1e-15):\n    \n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:25:36.481475Z","iopub.execute_input":"2024-02-22T18:25:36.482035Z","iopub.status.idle":"2024-02-22T18:25:36.493251Z","shell.execute_reply.started":"2024-02-22T18:25:36.481886Z","shell.execute_reply":"2024-02-22T18:25:36.492178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain=train.iloc[:14000]\nX_unlabeled=train.iloc[14000:]  #No need to worry about class imbalance here,as labels are dropped\nX_unlabeled=X_unlabeled.text.values\nytrain=y[:14000]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:27:14.620492Z","iopub.execute_input":"2024-02-22T18:27:14.620883Z","iopub.status.idle":"2024-02-22T18:27:14.628472Z","shell.execute_reply.started":"2024-02-22T18:27:14.620856Z","shell.execute_reply":"2024-02-22T18:27:14.626784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#splitting the dataset into train and validation \n\nXtrain,Xvalid,Ytrain,Yvalid = train_test_split(xtrain.text.values,ytrain,\n                                              stratify=ytrain,\n                                              random_state=42,\n                                              test_size=0.1,\n                                              shuffle=True)\nXtest=test.text.values\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:27:17.731915Z","iopub.execute_input":"2024-02-22T18:27:17.732309Z","iopub.status.idle":"2024-02-22T18:27:17.748669Z","shell.execute_reply.started":"2024-02-22T18:27:17.732282Z","shell.execute_reply":"2024-02-22T18:27:17.747717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Xtrain.shape)\nprint(Xvalid.shape)\nprint(X_unlabeled.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:17:02.201579Z","iopub.execute_input":"2024-02-22T18:17:02.202074Z","iopub.status.idle":"2024-02-22T18:17:02.210190Z","shell.execute_reply.started":"2024-02-22T18:17:02.202039Z","shell.execute_reply":"2024-02-22T18:17:02.207693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF ","metadata":{}},{"cell_type":"code","source":"# To convert terms to numeric value having some weights assign to them depending on the importance of the term\n\ntfv= TfidfVectorizer(min_df=3,\n                     max_features=None,\n                     strip_accents='unicode',\n                     analyzer='word',\n                     token_pattern=r'\\w{1,}',\n                     ngram_range=(1,3),\n                     use_idf=True,\n                     smooth_idf=True,\n                     sublinear_tf=True,                 \n                     stop_words='english'\n                    )\n\nX_train_unlabeled= np.concatenate((Xtrain,X_unlabeled))\n\ntfv.fit(list(Xtrain)+list(Xvalid)+list(X_unlabeled)+list(X_train_unlabeled))    \nxtrain_tfv=tfv.transform(Xtrain)\nx_unlabeled_tfv=tfv.transform(X_unlabeled)\nX_tfv=tfv.transform(X_train_unlabeled)\nxtest_tfv=tfv.transform(Xtest)                                   #---need to check!\nxvalid_tfv=tfv.transform(Xvalid)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:27:20.482052Z","iopub.execute_input":"2024-02-22T18:27:20.482760Z","iopub.status.idle":"2024-02-22T18:27:26.507926Z","shell.execute_reply.started":"2024-02-22T18:27:20.482716Z","shell.execute_reply":"2024-02-22T18:27:26.507039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fitting a simple naive bayes on TF-IDF\n\nclf=MultinomialNB()\nclf.fit(xtrain_tfv,Ytrain)\npseudo_labels=clf.predict(x_unlabeled_tfv)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:27:29.442537Z","iopub.execute_input":"2024-02-22T18:27:29.442978Z","iopub.status.idle":"2024-02-22T18:27:29.461112Z","shell.execute_reply.started":"2024-02-22T18:27:29.442934Z","shell.execute_reply":"2024-02-22T18:27:29.459109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_tfv=np.concatenate((Ytrain,pseudo_labels))\n\n\n#fit the model on new dataset\n\nclf_train_unlabeled=MultinomialNB()\nclf_train_unlabeled.fit(X_tfv,Y_tfv)\npredictions=clf_train_unlabeled.predict_proba(xvalid_tfv)\n\n# Evaluate the model\n\nprint('logloss: %0.3f '%multiclass_logloss(Yvalid,predictions))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:27:33.163690Z","iopub.execute_input":"2024-02-22T18:27:33.164149Z","iopub.status.idle":"2024-02-22T18:27:33.186191Z","shell.execute_reply.started":"2024-02-22T18:27:33.164114Z","shell.execute_reply":"2024-02-22T18:27:33.184884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CountVectorizer","metadata":{}},{"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nX_train_unlabeled= np.concatenate((Xtrain,X_unlabeled))\nctv.fit(list(Xtrain) + list(Xvalid)+list(X_unlabeled)+list(X_train_unlabeled))\nxtrain_ctv =  ctv.transform(Xtrain) \nxvalid_ctv = ctv.transform(Xvalid)\nx_unlabeled_ctv=ctv.transform(X_unlabeled)\nX_ctv=ctv.transform(X_train_unlabeled)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:27:39.991630Z","iopub.execute_input":"2024-02-22T18:27:39.992077Z","iopub.status.idle":"2024-02-22T18:27:47.064652Z","shell.execute_reply.started":"2024-02-22T18:27:39.992046Z","shell.execute_reply":"2024-02-22T18:27:47.063465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fitting a simple naive bayes on CountVectorizer\n\nclf=MultinomialNB()\nclf.fit(xtrain_ctv,Ytrain)\npseudo_labels=clf.predict(x_unlabeled_ctv)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:27:54.572635Z","iopub.execute_input":"2024-02-22T18:27:54.573127Z","iopub.status.idle":"2024-02-22T18:27:54.624263Z","shell.execute_reply.started":"2024-02-22T18:27:54.573096Z","shell.execute_reply":"2024-02-22T18:27:54.622704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Retrain model with pseudo labels | Semi-supervised learning\n\n\nY_ctv=np.concatenate((Ytrain,pseudo_labels))\n\n\n#fit the model on new dataset\n\nclf_train_unlabeled=MultinomialNB()\nclf_train_unlabeled.fit(X_ctv,Y_ctv)\npredictions=clf_train_unlabeled.predict_proba(xvalid_ctv)\n\n# Evaluate the model\n\nprint('logloss: %0.3f '%multiclass_logloss(Yvalid,predictions))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:27:56.837928Z","iopub.execute_input":"2024-02-22T18:27:56.838476Z","iopub.status.idle":"2024-02-22T18:27:56.906253Z","shell.execute_reply.started":"2024-02-22T18:27:56.838396Z","shell.execute_reply":"2024-02-22T18:27:56.904781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Observations\n\n* We observe a logloss of **0.611** (tfidf)/**0.530** (ctv) which is a bit higher than **0.57** (tfidf)/ **0.485** (ctv) observed when no semi supervised learning was used.It is important to note that we have used **self training** here to mimic the case when there is actual unlabeled dataset.\n* We observe no improvement in our model\n\n**Why have we seen no improvement?**\n\nWe were suppose to use semi supervised learning to avoid majorly two shortcomings that can arise in MultinomialNB:\n* Early convergence\n* Cold start issues\n\nBut using self training was not able to handle these issues because here the dataset size remains the same.","metadata":{}},{"cell_type":"markdown","source":"## Using scikit learn's self learning classifier\n\nNow we will use the self learning classifier which uses the concept of **pseudo label threshold** to label the unlabeled labels","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(train.text.values, y,\n                                                      stratify=y, \n                                                      random_state=42, \n                                                      test_size=0.2,\n                                                      shuffle=True\n                                                      )\n\n#mask 80% of labels within the training data and create a target variable that uses -1 to denote unlabeled data\n\n\n\ny_mask = np.random.rand(len(y_train)) < 0.95\n\ny_train[~y_mask] = -1\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:29:52.749320Z","iopub.execute_input":"2024-02-22T18:29:52.749693Z","iopub.status.idle":"2024-02-22T18:29:52.771931Z","shell.execute_reply.started":"2024-02-22T18:29:52.749664Z","shell.execute_reply":"2024-02-22T18:29:52.770959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_mask)\nprint(y_train)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:29:55.553865Z","iopub.execute_input":"2024-02-22T18:29:55.554733Z","iopub.status.idle":"2024-02-22T18:29:55.561573Z","shell.execute_reply.started":"2024-02-22T18:29:55.554700Z","shell.execute_reply":"2024-02-22T18:29:55.559982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logloss function\n\ndef multiclass_logloss(actual,predicted,eps=1e-15):\n    \n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:30:03.814143Z","iopub.execute_input":"2024-02-22T18:30:03.814567Z","iopub.status.idle":"2024-02-22T18:30:03.823955Z","shell.execute_reply.started":"2024-02-22T18:30:03.814534Z","shell.execute_reply":"2024-02-22T18:30:03.822139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TF-IDF","metadata":{}},{"cell_type":"code","source":"# apply tfidf\n\ntfv= TfidfVectorizer(min_df=3,\n                     max_features=None,\n                     strip_accents='unicode',\n                     analyzer='word',\n                     token_pattern=r'\\w{1,}',\n                     ngram_range=(1,3),\n                     use_idf=True,\n                     smooth_idf=True,\n                     sublinear_tf=True,                 \n                     stop_words='english'\n                    )\n\ntfv.fit(list(X_train)+list(X_valid))    \nxtrain_tfv=tfv.transform(X_train)\n#xtest_tfv=tfv.transform(Xtest)                                   #---need to check!\nxvalid_tfv=tfv.transform(X_valid)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:36:21.131782Z","iopub.execute_input":"2024-02-22T18:36:21.133128Z","iopub.status.idle":"2024-02-22T18:36:24.436647Z","shell.execute_reply.started":"2024-02-22T18:36:21.133081Z","shell.execute_reply":"2024-02-22T18:36:24.434687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=MultinomialNB()\n\nself_training_model = SelfTrainingClassifier(base_estimator=model, # An estimator object implementing fit and predict_proba.\n                                             threshold=0.70, # default=0.75, The decision threshold for use with criterion='threshold'. Should be in [0, 1).\n                                             criterion='threshold', # {‘threshold’, ‘k_best’}, default=’threshold’, The selection criterion used to select which labels to add to the training set. If 'threshold', pseudo-labels with prediction probabilities above threshold are added to the dataset. If 'k_best', the k_best pseudo-labels with highest prediction probabilities are added to the dataset.\n                                             max_iter=100,\n                                             verbose=True # default=False, Verbosity prints some information after each iteration\n                                             \n                                            )\n\nself_training_model.fit(xtrain_tfv,y_train)\ny_pred = self_training_model.predict_proba(xvalid_tfv)\n\n# Evaluate the model\n\nprint('logloss: %0.3f '%multiclass_logloss(y_valid,y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:36:27.554065Z","iopub.execute_input":"2024-02-22T18:36:27.554471Z","iopub.status.idle":"2024-02-22T18:36:27.609895Z","shell.execute_reply.started":"2024-02-22T18:36:27.554442Z","shell.execute_reply":"2024-02-22T18:36:27.608650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CountVectorize","metadata":{}},{"cell_type":"code","source":"#apply countvectorize\n\nctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(X_train) + list(X_valid))\nxtrain_ctv =  ctv.transform(X_train) \nxvalid_ctv = ctv.transform(X_valid)","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:30:10.650061Z","iopub.execute_input":"2024-02-22T18:30:10.651042Z","iopub.status.idle":"2024-02-22T18:30:15.036989Z","shell.execute_reply.started":"2024-02-22T18:30:10.650993Z","shell.execute_reply":"2024-02-22T18:30:15.035505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=MultinomialNB()\n\nself_training_model = SelfTrainingClassifier(base_estimator=model, # An estimator object implementing fit and predict_proba.\n                                             threshold=0.70, # default=0.75, The decision threshold for use with criterion='threshold'. Should be in [0, 1).\n                                             criterion='threshold', # {‘threshold’, ‘k_best’}, default=’threshold’, The selection criterion used to select which labels to add to the training set. If 'threshold', pseudo-labels with prediction probabilities above threshold are added to the dataset. If 'k_best', the k_best pseudo-labels with highest prediction probabilities are added to the dataset.\n                                             max_iter=100,\n                                             verbose=True # default=False, Verbosity prints some information after each iteration\n                                             \n                                            )\n\nself_training_model.fit(xtrain_ctv,y_train)\n#y_pred = self_training_model.predict_proba(xvalid_ctv)\ny_pred= self_training_model.predict_proba(xvalid_ctv)\n\n# Evaluate the model\n\nprint('logloss: %0.3f '%multiclass_logloss(y_valid,y_pred))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-22T18:30:27.300573Z","iopub.execute_input":"2024-02-22T18:30:27.301984Z","iopub.status.idle":"2024-02-22T18:30:27.547457Z","shell.execute_reply.started":"2024-02-22T18:30:27.301922Z","shell.execute_reply":"2024-02-22T18:30:27.546138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we observe a logloss of **0.466** which is improved from **0.48** from the base model(supervised learning), but here we note that we have masked only 5% of the data and unlabeled it to -1.When we we masked majority of the data or about 90% of the data as unlabeled the performance worsened ","metadata":{}}]}